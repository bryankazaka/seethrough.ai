{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f3f4f3",
   "metadata": {},
   "source": [
    "This script involves finetuning the SmolVLM2 model on the real/synthetic video video classification task. First we import all our relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import coremltools as ct\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "import subprocess\n",
    "import modelscope\n",
    "\n",
    "import argparse, os, random, tarfile, zipfile, subprocess, shutil\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import csv, requests\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import decord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f5b6b",
   "metadata": {},
   "source": [
    "Then we load the Genvideo dataset (500GB) which contains over one million diverse AI generated and real videos, from state of the art generative video models with temporal artefacts crucial for video-based AI detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53871da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"D:/GenVideo\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENV_BASE = \"https://www.modelscope.cn/datasets/cccnju/Gen-Video/resolve/master/\"\n",
    "K400_TXT   = \"https://s3.amazonaws.com/kinetics/400/train/k400_train_path.txt\"\n",
    "\n",
    "GENV_FILES = {\n",
    "    # real: MSR-VTT\n",
    "    #\"GenVideo-Val.zip\": 13.93,\n",
    "    # fake: AI generators\n",
    "    \"OpenSora.tar.gz\": 31.72,\n",
    "    \"Latte.split.aa\": 42.95, \"Latte.split.ab\": 15.85,\n",
    "    \"Pika.split.aa\": 42.95,  \"Pika.split.ab\": 15.62,\n",
    "    \"DynamicCrafter.split.aa\": 42.95, \"DynamicCrafter.split.ab\": 33.80,\n",
    "    \"SEINE.tar.gz\": 9.03,\n",
    "    \"SD.split.ah\": 24.96,\n",
    "    \"I2VGEN_XL.split.aj\":13.92,\n",
    "    \"SVD.split.ac\": 9.38\n",
    "\n",
    "}\n",
    "# Youku real splits\n",
    "YOUKU_PREFIX = \"Youku_1M_10s\"\n",
    "YOUKU_COUNT = [\n",
    "    \"aa\"\n",
    "]\n",
    "\n",
    "#K400_PARTS = [f\"train/part_{i:03}.tar.gz\" for i in range(4)] # first four parts only\n",
    "\n",
    "\n",
    "def _dl(url: str, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists(): return\n",
    "    resp = requests.get(url, stream=True)\n",
    "    resp.raise_for_status()\n",
    "    total = int(resp.headers.get(\"content-length\", 0))\n",
    "    with open(dst, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dst.name) as p:\n",
    "        for chunk in resp.iter_content(chunk_size=33_554_432):  # 4MB\n",
    "            if not chunk: break\n",
    "            f.write(chunk); p.update(len(chunk))\n",
    "\n",
    "def download_subset(root: Path):\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Full Kinetics-400 train parts\n",
    "    txt = requests.get(K400_TXT).text.splitlines()\n",
    "    for line in txt:\n",
    "        fname = Path(line).name\n",
    "        _dl(line, root/fname)\n",
    "\n",
    "    # 2) Youku-1M splits\n",
    "    for suf in YOUKU_COUNT:\n",
    "        fn = f\"{YOUKU_PREFIX}.split.{suf}\"\n",
    "        url = GENV_BASE + fn\n",
    "        _dl(url, root/fn)\n",
    "\n",
    "    # 3) AI archives\n",
    "    for fn in GENV_FILES:\n",
    "        _dl(GENV_BASE + fn, root/fn)\n",
    "\n",
    "def recombine_splits(pieces, tgt: Path, chunk_size=4_194_304):\n",
    "    with open(tgt, \"wb\") as w:\n",
    "        for p in pieces:\n",
    "            with open(p, \"rb\") as r:\n",
    "                shutil.copyfileobj(r, w, length=chunk_size)\n",
    "\n",
    "\n",
    "def _extract(path: Path, out: Path):\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    if path.suffix == \".zip\":\n",
    "        with zipfile.ZipFile(path) as z:\n",
    "            z.extractall(out)\n",
    "    else:\n",
    "        try:\n",
    "            # streaming mode\n",
    "            with tarfile.open(path, \"r|gz\") as stream:\n",
    "                for member in stream:\n",
    "                    try:\n",
    "                        stream.extract(member, out)\n",
    "                    except Exception as e:\n",
    "                        # skip any corrupt member\n",
    "                        print(f\"Skipping bad {member.name}: {e}\")\n",
    "        except (tarfile.ReadError, EOFError, OSError) as e:\n",
    "            # skip the entire archive if it's unreadable\n",
    "            print(f\"Skipping corrupt archive {path.name}: {e}\")\n",
    "\n",
    "\n",
    "def prepare_folders(root: Path):\n",
    "    fake_root = root / \"fake\"\n",
    "    real_root = root / \"real\"\n",
    "    fake_root.mkdir(exist_ok=True, parents=True)\n",
    "    real_root.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # 1) Handle MSR-VTT bundle\n",
    "    msrzip = root / \"GenVideo-Val.zip\"\n",
    "    if msrzip.exists():\n",
    "        tmp = root / \"_tmp_msr\"\n",
    "        if tmp.exists(): shutil.rmtree(tmp)\n",
    "        tmp.mkdir()\n",
    "        _extract(msrzip, tmp)\n",
    "        inner = next(tmp.iterdir())  \n",
    "        for cls, dest in [(\"Real\", real_root), (\"Fake\", fake_root)]:\n",
    "            src = inner / cls\n",
    "            if src.exists():\n",
    "                for child in src.iterdir():\n",
    "                    shutil.move(str(child), dest / child.name)\n",
    "        shutil.rmtree(tmp)\n",
    "\n",
    "    # 2) Extract full Kinetics-400 parts\n",
    "    kin_dest = real_root / \"Kinetics\"\n",
    "    kin_dest.mkdir(exist_ok=True)\n",
    "    for tar in sorted(root.glob(\"part_*.tar.gz\")):\n",
    "        _extract(tar, kin_dest)\n",
    "\n",
    "    # 3) Extract Youku-1M \n",
    "    yk_parts = sorted(root.glob(\"Youku_1M_10s.split.*\"))\n",
    "    if yk_parts:\n",
    "        yk_tar = root / \"Youku_1M_10s.tar.gz\"\n",
    "        recombine_splits(yk_parts, yk_tar)\n",
    "        _extract(yk_tar, real_root / \"Youku\")\n",
    "\n",
    "    # 4) Extract chosen fake sources\n",
    "    fake_sources = [\n",
    "        \"OpenSora.tar.gz\",\n",
    "        \"Latte.split.aa\", \"Latte.split.ab\",\n",
    "        \"Pika.split.aa\", \"Pika.split.ab\",\n",
    "        \"DynamicCrafter.split.aa\", \"DynamicCrafter.split.ab\",\n",
    "        \"SEINE.tar.gz\",\n",
    "        \"SD.split.ah\",\n",
    "        \"I2VGEN_XL.split.aj\",\n",
    "        \"SVD.split.ac\",\n",
    "    ]\n",
    "\n",
    "    for fn in fake_sources:\n",
    "        path = root / fn\n",
    "        stem = fn.split('.')[0]\n",
    "\n",
    "        # recombine if it's a split\n",
    "        if \".split.\" in fn:\n",
    "            parts = sorted(root.glob(f\"{stem}.split.*\"))\n",
    "            if parts:\n",
    "                tar_target = root / f\"{stem}.tar.gz\"\n",
    "                recombine_splits(parts, tar_target)\n",
    "                path = tar_target\n",
    "        # extract into fake/<stem>/\n",
    "        if path.exists():\n",
    "            _extract(path, fake_root / stem)\n",
    "\n",
    "\n",
    "def make_csv(root: Path, seed: int = 42):\n",
    "    # gather all clips\n",
    "    real_paths = list((root / \"real\").rglob(\"*.mp4\"))\n",
    "    fake_paths = list((root / \"fake\").rglob(\"*.mp4\"))\n",
    "\n",
    "    # determine balanced count\n",
    "    k = min(len(real_paths), len(fake_paths))\n",
    "    random.seed(seed)\n",
    "    real_sel = random.sample(real_paths, k)\n",
    "    fake_sel = random.sample(fake_paths, k)\n",
    "\n",
    "    all_sel = real_sel + fake_sel\n",
    "    random.shuffle(all_sel)\n",
    "\n",
    "    # split 80/10/10\n",
    "    n = len(all_sel)\n",
    "    train_end = int(0.8 * n)\n",
    "    val_end   = int(0.9 * n)\n",
    "    splits = {\n",
    "        \"train\": all_sel[:train_end],\n",
    "        \"val\"  : all_sel[train_end:val_end],\n",
    "        \"test\" : all_sel[val_end:]\n",
    "    }\n",
    "\n",
    "    # write CSVs\n",
    "    for name, items in splits.items():\n",
    "        csv_path = root / f\"{name}.csv\"\n",
    "        with open(csv_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"path\", \"label\"])\n",
    "            for p in items:\n",
    "                label = 0 if \"real\" in p.parts else 1\n",
    "                writer.writerow([p.as_posix(), label])\n",
    "\n",
    "    print(f\"Wrote splits: {', '.join([f'{k} ({len(v)})' for k,v in splits.items()])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38722b6a",
   "metadata": {},
   "source": [
    "Transforms and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"D:/GenVideo\")\n",
    "print(\"→ Downloading...\")\n",
    "#download_subset(root)\n",
    "print(\"→ Extracting...\")\n",
    "prepare_folders(root)\n",
    "print(\"→ Splitting...\")\n",
    "make_csv(root)\n",
    "print(\"✓  Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
